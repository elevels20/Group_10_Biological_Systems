{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0362d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.init as init\n",
    "\n",
    "#FIRST VARIANT OF THE HUMAN VISUAL SYSTEM\n",
    "class ReferenceVisualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward CNN with two parallel pathways (Ventral and Dorsal)\n",
    "    to serve as the baseline for the assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ReferenceVisualNetwork, self).__init__()\n",
    "        \n",
    "        # --- 1. Initial Feature Extraction (Shared V1/V2 - Early Visual Areas) ---\n",
    "        # Input size for Fashion MNIST: (1, 28, 28)\n",
    "        # These layers model the initial processing common to both pathways (V1, V2, Area hOc1, Area hOc2).\n",
    "        self.v1_v2_conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1) # (1, 28, 28) -> (32, 28, 28)\n",
    "        self.v1_v2_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                            # (32, 28, 28) -> (32, 14, 14)\n",
    "\n",
    "        self.v1_v2_conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # (32, 14, 14) -> (64, 14, 14)\n",
    "        self.v1_v2_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                            # (64, 14, 14) -> (64, 7, 7)\n",
    "        \n",
    "        # --- 2. Pathway Split and Processing ---\n",
    "        \n",
    "        # 2a. Ventral Pathway (The 'What' Pathway - hOc4d, hOc5)\n",
    "        # Characterized by more layers/filters for hierarchical feature extraction.\n",
    "        self.ventral_conv = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1) # (64, 7, 7) -> (128, 7, 7)\n",
    "        self.ventral_pool = nn.MaxPool2d(kernel_size=2, stride=2)                              # (128, 7, 7) -> (128, 3, 3)\n",
    "        \n",
    "        # Flattened size: 128 * 3 * 3 = 1152\n",
    "        self.ventral_fc = nn.Linear(128 * 3 * 3, 256)\n",
    "        \n",
    "        # 2b. Dorsal Pathway (The 'Where/How' Pathway - hOc3d)\n",
    "        # Characterized as being potentially shallower and focused on spatial/motion information.\n",
    "        self.dorsal_conv = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)  # (64, 7, 7) -> (64, 7, 7)\n",
    "        self.dorsal_pool = nn.MaxPool2d(kernel_size=3, stride=3)                                # (64, 7, 7) -> (64, 2, 2)\n",
    "        \n",
    "        # Flattened size: 64 * 2 * 2 = 256\n",
    "        self.dorsal_fc = nn.Linear(64 * 2 * 2, 128)\n",
    "        \n",
    "        # --- 3. Final Classification Layer ---\n",
    "        # Total concatenated features: 256 (Ventral) + 128 (Dorsal) = 384\n",
    "        self.classifier = nn.Linear(256 + 128, 10) # 10 classes for Fashion MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1. Initial Feature Extraction (Shared V1/V2)\n",
    "        x = self.v1_v2_pool1(F.relu(self.v1_v2_conv1(x)))\n",
    "        shared_features = self.v1_v2_pool2(F.relu(self.v1_v2_conv2(x)))\n",
    "        \n",
    "        # 2. Pathway Split\n",
    "        # 2a. Ventral Pathway\n",
    "        v = self.ventral_pool(F.relu(self.ventral_conv(shared_features)))\n",
    "        v = v.view(v.size(0), -1) # Flatten\n",
    "        v_out = F.relu(self.ventral_fc(v))\n",
    "        \n",
    "        # 2b. Dorsal Pathway\n",
    "        d = self.dorsal_pool(F.relu(self.dorsal_conv(shared_features)))\n",
    "        d = d.view(d.size(0), -1) # Flatten\n",
    "        d_out = F.relu(self.dorsal_fc(d))\n",
    "        \n",
    "        # 3. Concatenation and Classification\n",
    "        combined = torch.cat([v_out, d_out], dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        # The feature dictionary is needed later for RSA (Step 6).\n",
    "        return output, {'v1_v2': shared_features, 'ventral_fc': v_out, 'dorsal_fc': d_out}\n",
    "\n",
    "# --- Example of creating an instance (for testing/setup) ---\n",
    "# model = ReferenceVisualNetwork()\n",
    "# dummy_input = torch.randn(64, 1, 28, 28) # Batch size of 64\n",
    "# output = model(dummy_input)\n",
    "# print(f\"Output shape: {output.shape}\") # Should be (64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2da5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Densities (cells/0.1mm3): {'V1_left': 90.42985758514597, 'V1_right': 90.42985758514597, 'V2_right': 76.51516244077028, 'hOc3d_left': 72.07978610755333, 'hOc4d_right': 61.15705207380484, 'hOc5_right': 63.61690940972587}\n",
      "Minimum Density (D_min): 61.16\n",
      "\n",
      "--- FINAL SCALED FILTER COUNTS FOR CONSTRAINED MODEL ---\n",
      "{'V1_V2_CONV2_CHANNELS': 64, 'V1_V2_CONV1_CHANNELS': 32, 'DORSAL_CONV_CHANNELS': 64, 'DORSAL_FC_SIZE': 160, 'VENTRAL_CONV_CHANNELS': 64, 'VENTRAL_FC_SIZE': 256}\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_paths = {\n",
    "    'V1_left': 'Human_brain_data/Area hOc1 (V1, 17, CalcS) left - cell density/tabular.csv',\n",
    "    'V1_right': 'Human_brain_data/Area hOc1 (V1, 17, CalcS) right - cell density/tabular.csv',\n",
    "    'V2_right': 'Human_brain_data/Area hOc2 (V2, 18) right - cell density/tabular.csv',\n",
    "    'hOc3d_left': 'Human_brain_data/Area hOc3d (Cuneus) left - cell density/tabular.csv',\n",
    "    'hOc4d_right': 'Human_brain_data/Area hOc4d (Cuneus) right - cell density/tabular.csv',\n",
    "    'hOc5_right': 'Human_brain_data/Area hOc5 (LOC) right - cell density/tabular.csv',\n",
    "}\n",
    "\n",
    "# The column name from your readme file\n",
    "DENSITY_COL = 'Segmented cell body density (detected cells / 0.1mm3)'\n",
    "\n",
    "# Base filter count for the least dense region (F_base)\n",
    "BASE_FILTER_COUNT = 64\n",
    "# Scaling factor for shared layers (V1_conv1 is half the size of V1_conv2)\n",
    "SHARED_CONV1_RATIO = 0.5\n",
    "\n",
    "\n",
    "#CALCULATE AVERAGE DENSITIES\n",
    "\n",
    "avg_densities = {}\n",
    "all_densities = []\n",
    "\n",
    "for region, path in file_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            # Calculate the mean density across the 100 measurements\n",
    "            mean_density = df[DENSITY_COL].mean()\n",
    "            avg_densities[region] = mean_density\n",
    "            all_densities.append(mean_density)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "            avg_densities[region] = np.nan\n",
    "    else:\n",
    "        print(f\"File not found for {region}. Please check path: {path}\")\n",
    "        avg_densities[region] = np.nan\n",
    "        \n",
    "# Filter out NaNs if files were missing\n",
    "valid_densities = [d for d in all_densities if not np.isnan(d)]\n",
    "\n",
    "if not valid_densities:\n",
    "    raise ValueError(\"No valid density data was loaded. Please fix file paths.\")\n",
    "\n",
    "# Find the minimum density (D_min) for scaling\n",
    "MIN_DENSITY = min(valid_densities)\n",
    "\n",
    "print(f\"Average Densities (cells/0.1mm3): {avg_densities}\")\n",
    "print(f\"Minimum Density (D_min): {MIN_DENSITY:.2f}\")\n",
    "\n",
    "\n",
    "#MAP DENSITY TO ARCHITECTURAL COMPONENTS\\\n",
    "\n",
    "# 1. Shared V1/V2 (Averaging V1 left, V1 right, V2 right)\n",
    "shared_v1_v2_avg_density = np.mean([\n",
    "    avg_densities['V1_left'], avg_densities['V1_right'], avg_densities['V2_right']\n",
    "])\n",
    "# 2. Dorsal Pathway (hOc3d left)\n",
    "dorsal_avg_density = avg_densities['hOc3d_left']\n",
    "# 3. Ventral Pathway (Averaging hOc4d right, hOc5 right)\n",
    "ventral_avg_density = np.mean([\n",
    "    avg_densities['hOc4d_right'], avg_densities['hOc5_right']\n",
    "])\n",
    "\n",
    "# Calculate scaling factors\n",
    "scaling_factors = {\n",
    "    'Shared_V1_V2': shared_v1_v2_avg_density / MIN_DENSITY,\n",
    "    'Dorsal': dorsal_avg_density / MIN_DENSITY,\n",
    "    'Ventral': ventral_avg_density / MIN_DENSITY,\n",
    "}\n",
    "\n",
    "#\\CALCULATE FINAL FILTER COUNTS\\\n",
    "\n",
    "# Calculate the raw size for V1_V2_CONV2_CHANNELS before rounding\n",
    "conv2_size_raw = scaling_factors['Shared_V1_V2'] * BASE_FILTER_COUNT\n",
    "\n",
    "DENSITY_SCALED_FILTERS = {\n",
    "    # Shared V1/V2 Layers\n",
    "    # Uses the raw calculated size\n",
    "    'V1_V2_CONV2_CHANNELS': int(np.round(conv2_size_raw)),\n",
    "    # Uses the raw calculated size multiplied by 0.5 ratio\n",
    "    'V1_V2_CONV1_CHANNELS': int(np.round(conv2_size_raw * SHARED_CONV1_RATIO)),\n",
    "    \n",
    "    # Dorsal Pathway\n",
    "    'DORSAL_CONV_CHANNELS': int(np.round(scaling_factors['Dorsal'] * BASE_FILTER_COUNT)),\n",
    "    'DORSAL_FC_SIZE': int(np.round(scaling_factors['Dorsal'] * (BASE_FILTER_COUNT * 2))), \n",
    "    \n",
    "    # Ventral Pathway\n",
    "    'VENTRAL_CONV_CHANNELS': int(np.round(scaling_factors['Ventral'] * BASE_FILTER_COUNT)),\n",
    "    'VENTRAL_FC_SIZE': int(np.round(scaling_factors['Ventral'] * (BASE_FILTER_COUNT * 4))),\n",
    "}\n",
    "\n",
    "# Ensure all channel counts are at least 1 and round to nearest power of 2 for CNN efficiency\n",
    "def round_to_power_of_2(n):\n",
    "    if n <= 0: return 2\n",
    "    return int(2**np.round(np.log2(n)))\n",
    "\n",
    "for key in DENSITY_SCALED_FILTERS:\n",
    "    if 'CHANNELS' in key:\n",
    "        # Round convolutional channel counts to powers of 2 (32, 64, 128, etc.)\n",
    "        DENSITY_SCALED_FILTERS[key] = round_to_power_of_2(DENSITY_SCALED_FILTERS[key])\n",
    "    elif 'SIZE' in key:\n",
    "        # Keep FC sizes as general integers, rounding to nearest 32\n",
    "        DENSITY_SCALED_FILTERS[key] = max(32, int(np.round(DENSITY_SCALED_FILTERS[key] / 32) * 32))\n",
    "\n",
    "# Recalculate V1_V2_CONV1 based on the FINAL, rounded V1_V2_CONV2 value\n",
    "DENSITY_SCALED_FILTERS['V1_V2_CONV1_CHANNELS'] = round_to_power_of_2(\n",
    "    DENSITY_SCALED_FILTERS['V1_V2_CONV2_CHANNELS'] // 2\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- FINAL SCALED FILTER COUNTS FOR CONSTRAINED MODEL ---\")\n",
    "print(DENSITY_SCALED_FILTERS)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3552ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "V1_CONV1_C = 32    # V1/V2 CONV1 channels\n",
    "V1_CONV2_C = 64    # V1/V2 CONV2 channels\n",
    "\n",
    "D_CONV_C = 64       # Dorsal Conv channels (hOc3d)\n",
    "D_FC_S = 160       # Dorsal FC size\n",
    "\n",
    "V_CONV_C = 64     # Ventral Conv channels (hOc4d/hOc5)\n",
    "V_FC_S = 256      # Ventral FC size\n",
    "\n",
    "#SECOND VARIANT OF THE HUMAN VISUAL SYSTEM\n",
    "class ConstrainedVisualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A network constrained by human neuroanatomical data (cell density and SC).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConstrainedVisualNetwork, self).__init__()\n",
    "        \n",
    "        # --- 1. Shared V1/V2 (Size constrained by V1/V2 density) ---\n",
    "        self.v1_v2_conv1 = nn.Conv2d(1, V1_CONV1_C, kernel_size=3, padding=1)\n",
    "        self.v1_v2_pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.v1_v2_conv2 = nn.Conv2d(V1_CONV1_C, V1_CONV2_C, kernel_size=3, padding=1)\n",
    "        self.v1_v2_pool2 = nn.MaxPool2d(2, 2) # Output size: (V1_CONV2_C, 7, 7)\n",
    "        \n",
    "        # --- 2. Ventral Pathway (hOc4d, hOc5 - Size constrained by density) ---\n",
    "        self.ventral_conv = nn.Conv2d(V1_CONV2_C, V_CONV_C, kernel_size=3, padding=1)\n",
    "        self.ventral_pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Flattened size: V_CONV_C * 3 * 3\n",
    "        VENTRAL_FLATTEN_SIZE = V_CONV_C * 3 * 3\n",
    "        self.ventral_fc = nn.Linear(VENTRAL_FLATTEN_SIZE, V_FC_S)\n",
    "        \n",
    "        # --- 3. Dorsal Pathway (hOc3d - Size constrained by density) ---\n",
    "        self.dorsal_conv = nn.Conv2d(V1_CONV2_C, D_CONV_C, kernel_size=3, padding=1)\n",
    "        self.dorsal_pool = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        # Flattened size: D_CONV_C * 2 * 2\n",
    "        DORSAL_FLATTEN_SIZE = D_CONV_C * 2 * 2\n",
    "        self.dorsal_fc = nn.Linear(DORSAL_FLATTEN_SIZE, D_FC_S)\n",
    "        \n",
    "        # --- 4. Structural Connectivity Module (SC) ---\n",
    "        # This layer transforms V1_CONV1_C features (from V1_conv1 output) to match \n",
    "        # the spatial/channel dimensions of the Dorsal input (V1_CONV2_C, 7, 7).\n",
    "        # Adjust the input/output channels and stride based on your SC data (e.g., V1->hOc3d).\n",
    "        self.v1_to_dorsal_skip = nn.Conv2d(V1_CONV1_C, V1_CONV2_C, kernel_size=1, stride=4, bias=False) \n",
    "        \n",
    "        # 5. Final Classifier\n",
    "        self.classifier = nn.Linear(V_FC_S + D_FC_S, 10) \n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initializes weights for stability, matching the template provided earlier\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, 0, 0.01)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1. Initial V1 processing (Source for SC skip connection)\n",
    "        v1_features = F.relu(self.v1_v2_conv1(x)) # (B, V1_CONV1_C, 28, 28)\n",
    "        x = self.v1_v2_pool1(v1_features)         # (B, V1_CONV1_C, 14, 14)\n",
    "        \n",
    "        # 2. V2 Processing (Shared features)\n",
    "        shared_features = self.v1_v2_pool2(F.relu(self.v1_v2_conv2(x))) # (B, V1_CONV2_C, 7, 7)\n",
    "        \n",
    "        # 3. Ventral Pathway\n",
    "        v = self.ventral_pool(F.relu(self.ventral_conv(shared_features)))\n",
    "        ventral_conv_out = v \n",
    "        \n",
    "        # --- CRITICAL: STRUCTURAL CONNECTIVITY (SC) IMPLEMENTATION ---\n",
    "        \n",
    "        # Prepare SC skip: Transforms V1 features (V1_CONV1_C, 28, 28) \n",
    "        # to match the shape of the dorsal input (V1_CONV2_C, 7, 7).\n",
    "        v1_skip = self.v1_to_dorsal_skip(v1_features) \n",
    "\n",
    "        # 4. Dorsal Pathway with SC Constraint\n",
    "        # Input to dorsal pathway = Shared V2 features + Direct V1 Skip (Residual connection)\n",
    "        # \n",
    "        # !!! CHECK YOUR CONNECTIVITY DATA AND ADJUST THE ADDITION BELOW !!!\n",
    "        # If your data shows a strong V1 -> hOc3d connection, keep this:\n",
    "        d_input = shared_features + v1_skip\n",
    "        \n",
    "        d = self.dorsal_pool(F.relu(self.dorsal_conv(d_input))) \n",
    "        \n",
    "        # Flatten and FC for both pathways\n",
    "        v_fc_in = ventral_conv_out.view(ventral_conv_out.size(0), -1)\n",
    "        v_out = F.relu(self.ventral_fc(v_fc_in))\n",
    "        \n",
    "        d_fc_in = d.view(d.size(0), -1)\n",
    "        d_out = F.relu(self.dorsal_fc(d_fc_in))\n",
    "        \n",
    "        # 5. Concatenation and Classification\n",
    "        combined = torch.cat([v_out, d_out], dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        return output, {'v1_v2': shared_features, 'ventral_fc': v_out, 'dorsal_fc': d_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcf47f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 10 \n",
    "\n",
    "def load_fashion_mnist():\n",
    "    \"\"\"Loads and prepares the Fashion MNIST dataset.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Download and load training dataset\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Split training set into training and validation\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Download and load test dataset\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=N_EPOCHS, lr=LEARNING_RATE, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Trains the given model and tracks loss/accuracy.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"Starting training for {type(model).__name__} on {device}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # The forward pass now returns output AND features\n",
    "            outputs, _ = model(inputs) \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Training finished in {(end_time - start_time):.2f} seconds.\")\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"Evaluates the model on the given data loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b72d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:05<00:00, 5.10MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 1.10MB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.59MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Reference Network ---\n",
      "Starting training for ReferenceVisualNetwork on cpu...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training Reference Network ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m ref_model \u001b[38;5;241m=\u001b[39m ReferenceVisualNetwork()\n\u001b[1;32m----> 7\u001b[0m ref_model_trained, ref_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(ref_model_trained\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 3. Train the Constrained Model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 63\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# The forward pass now returns output AND features\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs) \n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Setup and Data Loading\n",
    "train_loader, val_loader, test_loader = load_fashion_mnist() # This function was provided earlier\n",
    "\n",
    "# 2. Train the Reference Model\n",
    "print(\"--- Training Reference Network ---\")\n",
    "ref_model = ReferenceVisualNetwork()\n",
    "ref_model_trained, ref_history = train_model(ref_model, train_loader, val_loader)\n",
    "torch.save(ref_model_trained.state_dict(), 'reference_model.pth') # Save the model\n",
    "\n",
    "# 3. Train the Constrained Model\n",
    "print(\"\\n--- Training Constrained Network ---\")\n",
    "constrained_model = ConstrainedVisualNetwork() \n",
    "constrained_model_trained, const_history = train_model(constrained_model, train_loader, val_loader)\n",
    "torch.save(constrained_model_trained.state_dict(), 'constrained_model.pth') # Save the model\n",
    "\n",
    "print(\"\\nModels trained and saved. Ready for Step 6: RSA.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
