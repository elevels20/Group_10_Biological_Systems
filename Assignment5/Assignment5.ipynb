{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0362d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import spearmanr\n",
    "import json\n",
    "\n",
    "\n",
    "#FIRST VARIANT OF THE HUMAN VISUAL SYSTEM\n",
    "class ReferenceVisualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward CNN with two parallel pathways (Ventral and Dorsal)\n",
    "    to serve as the baseline for the assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ReferenceVisualNetwork, self).__init__()\n",
    "        \n",
    "        # --- 1. Initial Feature Extraction (Shared V1/V2 - Early Visual Areas) ---\n",
    "        # Input size for Fashion MNIST: (1, 28, 28)\n",
    "        # These layers model the initial processing common to both pathways (V1, V2, Area hOc1, Area hOc2).\n",
    "        self.v1_v2_conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1) # (1, 28, 28) -> (32, 28, 28)\n",
    "        self.v1_v2_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                            # (32, 28, 28) -> (32, 14, 14)\n",
    "\n",
    "        self.v1_v2_conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # (32, 14, 14) -> (64, 14, 14)\n",
    "        self.v1_v2_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                            # (64, 14, 14) -> (64, 7, 7)\n",
    "        \n",
    "        # --- 2. Pathway Split and Processing ---\n",
    "        \n",
    "        # 2a. Ventral Pathway (The 'What' Pathway - hOc4d, hOc5)\n",
    "        # Characterized by more layers/filters for hierarchical feature extraction.\n",
    "        self.ventral_conv = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1) # (64, 7, 7) -> (128, 7, 7)\n",
    "        self.ventral_pool = nn.MaxPool2d(kernel_size=2, stride=2)                              # (128, 7, 7) -> (128, 3, 3)\n",
    "        \n",
    "        # Flattened size: 128 * 3 * 3 = 1152\n",
    "        self.ventral_fc = nn.Linear(128 * 3 * 3, 256)\n",
    "        \n",
    "        # 2b. Dorsal Pathway (The 'Where/How' Pathway - hOc3d)\n",
    "        # Characterized as being potentially shallower and focused on spatial/motion information.\n",
    "        self.dorsal_conv = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)  # (64, 7, 7) -> (64, 7, 7)\n",
    "        self.dorsal_pool = nn.MaxPool2d(kernel_size=3, stride=3)                                # (64, 7, 7) -> (64, 2, 2)\n",
    "        \n",
    "        # Flattened size: 64 * 2 * 2 = 256\n",
    "        self.dorsal_fc = nn.Linear(64 * 2 * 2, 128)\n",
    "        \n",
    "        # --- 3. Final Classification Layer ---\n",
    "        # Total concatenated features: 256 (Ventral) + 128 (Dorsal) = 384\n",
    "        self.classifier = nn.Linear(256 + 128, 10) # 10 classes for Fashion MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1. Initial Feature Extraction (Shared V1/V2)\n",
    "        x = self.v1_v2_pool1(F.relu(self.v1_v2_conv1(x)))\n",
    "        shared_features = self.v1_v2_pool2(F.relu(self.v1_v2_conv2(x)))\n",
    "        \n",
    "        # 2. Pathway Split\n",
    "        # 2a. Ventral Pathway\n",
    "        v = self.ventral_pool(F.relu(self.ventral_conv(shared_features)))\n",
    "        v = v.view(v.size(0), -1) # Flatten\n",
    "        v_out = F.relu(self.ventral_fc(v))\n",
    "        \n",
    "        # 2b. Dorsal Pathway\n",
    "        d = self.dorsal_pool(F.relu(self.dorsal_conv(shared_features)))\n",
    "        d = d.view(d.size(0), -1) # Flatten\n",
    "        d_out = F.relu(self.dorsal_fc(d))\n",
    "        \n",
    "        # 3. Concatenation and Classification\n",
    "        combined = torch.cat([v_out, d_out], dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        # The feature dictionary is needed later for RSA (Step 6).\n",
    "        return output, {'v1_v2': shared_features, 'ventral_fc': v_out, 'dorsal_fc': d_out}\n",
    "\n",
    "# --- Example of creating an instance (for testing/setup) ---\n",
    "# model = ReferenceVisualNetwork()\n",
    "# dummy_input = torch.randn(64, 1, 28, 28) # Batch size of 64\n",
    "# output = model(dummy_input)\n",
    "# print(f\"Output shape: {output.shape}\") # Should be (64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2da5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Densities (cells/0.1mm3): {'V1_left': 90.42985758514597, 'V1_right': 90.42985758514597, 'V2_right': 76.51516244077028, 'hOc3d_left': 72.07978610755333, 'hOc4d_right': 61.15705207380484, 'hOc5_right': 63.61690940972587}\n",
      "Minimum Density (D_min): 61.16\n",
      "\n",
      "--- FINAL SCALED FILTER COUNTS FOR CONSTRAINED MODEL ---\n",
      "{'V1_V2_CONV2_CHANNELS': 64, 'V1_V2_CONV1_CHANNELS': 32, 'DORSAL_CONV_CHANNELS': 64, 'DORSAL_FC_SIZE': 160, 'VENTRAL_CONV_CHANNELS': 64, 'VENTRAL_FC_SIZE': 256}\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_paths = {\n",
    "    'V1_left': 'Human_brain_data/Area hOc1 (V1, 17, CalcS) left - cell density/tabular.csv',\n",
    "    'V1_right': 'Human_brain_data/Area hOc1 (V1, 17, CalcS) right - cell density/tabular.csv',\n",
    "    'V2_right': 'Human_brain_data/Area hOc2 (V2, 18) right - cell density/tabular.csv',\n",
    "    'hOc3d_left': 'Human_brain_data/Area hOc3d (Cuneus) left - cell density/tabular.csv',\n",
    "    'hOc4d_right': 'Human_brain_data/Area hOc4d (Cuneus) right - cell density/tabular.csv',\n",
    "    'hOc5_right': 'Human_brain_data/Area hOc5 (LOC) right - cell density/tabular.csv',\n",
    "}\n",
    "\n",
    "# The column name from your readme file\n",
    "DENSITY_COL = 'Segmented cell body density (detected cells / 0.1mm3)'\n",
    "\n",
    "# Base filter count for the least dense region (F_base)\n",
    "BASE_FILTER_COUNT = 64\n",
    "# Scaling factor for shared layers (V1_conv1 is half the size of V1_conv2)\n",
    "SHARED_CONV1_RATIO = 0.5\n",
    "\n",
    "\n",
    "#CALCULATE AVERAGE DENSITIES\n",
    "\n",
    "avg_densities = {}\n",
    "all_densities = []\n",
    "\n",
    "for region, path in file_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            # Calculate the mean density across the 100 measurements\n",
    "            mean_density = df[DENSITY_COL].mean()\n",
    "            avg_densities[region] = mean_density\n",
    "            all_densities.append(mean_density)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "            avg_densities[region] = np.nan\n",
    "    else:\n",
    "        print(f\"File not found for {region}. Please check path: {path}\")\n",
    "        avg_densities[region] = np.nan\n",
    "        \n",
    "# Filter out NaNs if files were missing\n",
    "valid_densities = [d for d in all_densities if not np.isnan(d)]\n",
    "\n",
    "if not valid_densities:\n",
    "    raise ValueError(\"No valid density data was loaded. Please fix file paths.\")\n",
    "\n",
    "# Find the minimum density (D_min) for scaling\n",
    "MIN_DENSITY = min(valid_densities)\n",
    "\n",
    "print(f\"Average Densities (cells/0.1mm3): {avg_densities}\")\n",
    "print(f\"Minimum Density (D_min): {MIN_DENSITY:.2f}\")\n",
    "\n",
    "\n",
    "#MAP DENSITY TO ARCHITECTURAL COMPONENTS\\\n",
    "\n",
    "# 1. Shared V1/V2 (Averaging V1 left, V1 right, V2 right)\n",
    "shared_v1_v2_avg_density = np.mean([\n",
    "    avg_densities['V1_left'], avg_densities['V1_right'], avg_densities['V2_right']\n",
    "])\n",
    "# 2. Dorsal Pathway (hOc3d left)\n",
    "dorsal_avg_density = avg_densities['hOc3d_left']\n",
    "# 3. Ventral Pathway (Averaging hOc4d right, hOc5 right)\n",
    "ventral_avg_density = np.mean([\n",
    "    avg_densities['hOc4d_right'], avg_densities['hOc5_right']\n",
    "])\n",
    "\n",
    "# Calculate scaling factors\n",
    "scaling_factors = {\n",
    "    'Shared_V1_V2': shared_v1_v2_avg_density / MIN_DENSITY,\n",
    "    'Dorsal': dorsal_avg_density / MIN_DENSITY,\n",
    "    'Ventral': ventral_avg_density / MIN_DENSITY,\n",
    "}\n",
    "\n",
    "#\\CALCULATE FINAL FILTER COUNTS\\\n",
    "\n",
    "# Calculate the raw size for V1_V2_CONV2_CHANNELS before rounding\n",
    "conv2_size_raw = scaling_factors['Shared_V1_V2'] * BASE_FILTER_COUNT\n",
    "\n",
    "DENSITY_SCALED_FILTERS = {\n",
    "    # Shared V1/V2 Layers\n",
    "    # Uses the raw calculated size\n",
    "    'V1_V2_CONV2_CHANNELS': int(np.round(conv2_size_raw)),\n",
    "    # Uses the raw calculated size multiplied by 0.5 ratio\n",
    "    'V1_V2_CONV1_CHANNELS': int(np.round(conv2_size_raw * SHARED_CONV1_RATIO)),\n",
    "    \n",
    "    # Dorsal Pathway\n",
    "    'DORSAL_CONV_CHANNELS': int(np.round(scaling_factors['Dorsal'] * BASE_FILTER_COUNT)),\n",
    "    'DORSAL_FC_SIZE': int(np.round(scaling_factors['Dorsal'] * (BASE_FILTER_COUNT * 2))), \n",
    "    \n",
    "    # Ventral Pathway\n",
    "    'VENTRAL_CONV_CHANNELS': int(np.round(scaling_factors['Ventral'] * BASE_FILTER_COUNT)),\n",
    "    'VENTRAL_FC_SIZE': int(np.round(scaling_factors['Ventral'] * (BASE_FILTER_COUNT * 4))),\n",
    "}\n",
    "\n",
    "# Ensure all channel counts are at least 1 and round to nearest power of 2 for CNN efficiency\n",
    "def round_to_power_of_2(n):\n",
    "    if n <= 0: return 2\n",
    "    return int(2**np.round(np.log2(n)))\n",
    "\n",
    "for key in DENSITY_SCALED_FILTERS:\n",
    "    if 'CHANNELS' in key:\n",
    "        # Round convolutional channel counts to powers of 2 (32, 64, 128, etc.)\n",
    "        DENSITY_SCALED_FILTERS[key] = round_to_power_of_2(DENSITY_SCALED_FILTERS[key])\n",
    "    elif 'SIZE' in key:\n",
    "        # Keep FC sizes as general integers, rounding to nearest 32\n",
    "        DENSITY_SCALED_FILTERS[key] = max(32, int(np.round(DENSITY_SCALED_FILTERS[key] / 32) * 32))\n",
    "\n",
    "# Recalculate V1_V2_CONV1 based on the FINAL, rounded V1_V2_CONV2 value\n",
    "DENSITY_SCALED_FILTERS['V1_V2_CONV1_CHANNELS'] = round_to_power_of_2(\n",
    "    DENSITY_SCALED_FILTERS['V1_V2_CONV2_CHANNELS'] // 2\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- FINAL SCALED FILTER COUNTS FOR CONSTRAINED MODEL ---\")\n",
    "print(DENSITY_SCALED_FILTERS)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3552ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "V1_CONV1_C = 32    # V1/V2 CONV1 channels\n",
    "V1_CONV2_C = 64    # V1/V2 CONV2 channels\n",
    "\n",
    "D_CONV_C = 64       # Dorsal Conv channels (hOc3d)\n",
    "D_FC_S = 160       # Dorsal FC size\n",
    "\n",
    "V_CONV_C = 64     # Ventral Conv channels (hOc4d/hOc5)\n",
    "V_FC_S = 256      # Ventral FC size\n",
    "\n",
    "#SECOND VARIANT OF THE HUMAN VISUAL SYSTEM\n",
    "class ConstrainedVisualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A network constrained by human neuroanatomical data (cell density and SC).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConstrainedVisualNetwork, self).__init__()\n",
    "        \n",
    "        # --- 1. Shared V1/V2 (Size constrained by V1/V2 density) ---\n",
    "        self.v1_v2_conv1 = nn.Conv2d(1, V1_CONV1_C, kernel_size=3, padding=1)\n",
    "        self.v1_v2_pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.v1_v2_conv2 = nn.Conv2d(V1_CONV1_C, V1_CONV2_C, kernel_size=3, padding=1)\n",
    "        self.v1_v2_pool2 = nn.MaxPool2d(2, 2) # Output size: (V1_CONV2_C, 7, 7)\n",
    "        \n",
    "        # --- 2. Ventral Pathway (hOc4d, hOc5 - Size constrained by density) ---\n",
    "        self.ventral_conv = nn.Conv2d(V1_CONV2_C, V_CONV_C, kernel_size=3, padding=1)\n",
    "        self.ventral_pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Flattened size: V_CONV_C * 3 * 3\n",
    "        VENTRAL_FLATTEN_SIZE = V_CONV_C * 3 * 3\n",
    "        self.ventral_fc = nn.Linear(VENTRAL_FLATTEN_SIZE, V_FC_S)\n",
    "        \n",
    "        # --- 3. Dorsal Pathway (hOc3d - Size constrained by density) ---\n",
    "        self.dorsal_conv = nn.Conv2d(V1_CONV2_C, D_CONV_C, kernel_size=3, padding=1)\n",
    "        self.dorsal_pool = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        # Flattened size: D_CONV_C * 2 * 2\n",
    "        DORSAL_FLATTEN_SIZE = D_CONV_C * 2 * 2\n",
    "        self.dorsal_fc = nn.Linear(DORSAL_FLATTEN_SIZE, D_FC_S)\n",
    "        \n",
    "        # --- 4. Structural Connectivity Module (SC) ---\n",
    "        # This layer transforms V1_CONV1_C features (from V1_conv1 output) to match \n",
    "        # the spatial/channel dimensions of the Dorsal input (V1_CONV2_C, 7, 7).\n",
    "        # Adjust the input/output channels and stride based on your SC data (e.g., V1->hOc3d).\n",
    "        self.v1_to_dorsal_skip = nn.Conv2d(V1_CONV1_C, V1_CONV2_C, kernel_size=1, stride=4, bias=False) \n",
    "        \n",
    "        # 5. Final Classifier\n",
    "        self.classifier = nn.Linear(V_FC_S + D_FC_S, 10) \n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initializes weights for stability, matching the template provided earlier\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, 0, 0.01)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1. Initial V1 processing (Source for SC skip connection)\n",
    "        v1_features = F.relu(self.v1_v2_conv1(x)) # (B, V1_CONV1_C, 28, 28)\n",
    "        x = self.v1_v2_pool1(v1_features)         # (B, V1_CONV1_C, 14, 14)\n",
    "        \n",
    "        # 2. V2 Processing (Shared features)\n",
    "        shared_features = self.v1_v2_pool2(F.relu(self.v1_v2_conv2(x))) # (B, V1_CONV2_C, 7, 7)\n",
    "        \n",
    "        # 3. Ventral Pathway\n",
    "        v = self.ventral_pool(F.relu(self.ventral_conv(shared_features)))\n",
    "        ventral_conv_out = v \n",
    "        \n",
    "        # --- CRITICAL: STRUCTURAL CONNECTIVITY (SC) IMPLEMENTATION ---\n",
    "        \n",
    "        # Prepare SC skip: Transforms V1 features (V1_CONV1_C, 28, 28) \n",
    "        # to match the shape of the dorsal input (V1_CONV2_C, 7, 7).\n",
    "        v1_skip = self.v1_to_dorsal_skip(v1_features) \n",
    "\n",
    "        # 4. Dorsal Pathway with SC Constraint\n",
    "        # Input to dorsal pathway = Shared V2 features + Direct V1 Skip (Residual connection)\n",
    "        # \n",
    "        # !!! CHECK YOUR CONNECTIVITY DATA AND ADJUST THE ADDITION BELOW !!!\n",
    "        # If your data shows a strong V1 -> hOc3d connection, keep this:\n",
    "        d_input = shared_features + v1_skip\n",
    "        \n",
    "        d = self.dorsal_pool(F.relu(self.dorsal_conv(d_input))) \n",
    "        \n",
    "        # Flatten and FC for both pathways\n",
    "        v_fc_in = ventral_conv_out.view(ventral_conv_out.size(0), -1)\n",
    "        v_out = F.relu(self.ventral_fc(v_fc_in))\n",
    "        \n",
    "        d_fc_in = d.view(d.size(0), -1)\n",
    "        d_out = F.relu(self.dorsal_fc(d_fc_in))\n",
    "        \n",
    "        # 5. Concatenation and Classification\n",
    "        combined = torch.cat([v_out, d_out], dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        return output, {'v1_v2': shared_features, 'ventral_fc': v_out, 'dorsal_fc': d_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf47f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 10 \n",
    "\n",
    "def load_fashion_mnist():\n",
    "    \"\"\"Loads and prepares the Fashion MNIST dataset.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Download and load training dataset\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Split training set into training and validation\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Download and load test dataset\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=N_EPOCHS, lr=LEARNING_RATE, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Trains the given model and tracks loss/accuracy.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"Starting training for {type(model).__name__} on {device}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # The forward pass now returns output AND features\n",
    "            outputs, _ = model(inputs) \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Training finished in {(end_time - start_time):.2f} seconds.\")\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"Evaluates the model on the given data loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b72d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:13<00:00, 1.94MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 129kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:02<00:00, 1.88MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 26.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Reference Network ---\n",
      "Starting training for ReferenceVisualNetwork on cpu...\n",
      "Epoch 1/10: Train Loss: 0.5544 | Val Loss: 0.3608 | Val Acc: 0.8667\n",
      "Epoch 2/10: Train Loss: 0.3196 | Val Loss: 0.2901 | Val Acc: 0.8900\n",
      "Epoch 3/10: Train Loss: 0.2701 | Val Loss: 0.2483 | Val Acc: 0.9073\n",
      "Epoch 4/10: Train Loss: 0.2365 | Val Loss: 0.2590 | Val Acc: 0.9025\n",
      "Epoch 5/10: Train Loss: 0.2118 | Val Loss: 0.2461 | Val Acc: 0.9068\n",
      "Epoch 6/10: Train Loss: 0.1903 | Val Loss: 0.2494 | Val Acc: 0.9097\n",
      "Epoch 7/10: Train Loss: 0.1697 | Val Loss: 0.2248 | Val Acc: 0.9168\n",
      "Epoch 8/10: Train Loss: 0.1521 | Val Loss: 0.2243 | Val Acc: 0.9177\n",
      "Epoch 9/10: Train Loss: 0.1377 | Val Loss: 0.2467 | Val Acc: 0.9115\n",
      "Epoch 10/10: Train Loss: 0.1213 | Val Loss: 0.2294 | Val Acc: 0.9201\n",
      "Training finished in 306.83 seconds.\n",
      "\n",
      "--- Training Constrained Network ---\n",
      "Starting training for ConstrainedVisualNetwork on cpu...\n",
      "Epoch 1/10: Train Loss: 0.6160 | Val Loss: 0.3845 | Val Acc: 0.8609\n",
      "Epoch 2/10: Train Loss: 0.3486 | Val Loss: 0.3076 | Val Acc: 0.8869\n",
      "Epoch 3/10: Train Loss: 0.2905 | Val Loss: 0.2679 | Val Acc: 0.9002\n",
      "Epoch 4/10: Train Loss: 0.2469 | Val Loss: 0.2614 | Val Acc: 0.9024\n",
      "Epoch 5/10: Train Loss: 0.2243 | Val Loss: 0.2432 | Val Acc: 0.9114\n",
      "Epoch 6/10: Train Loss: 0.1999 | Val Loss: 0.2410 | Val Acc: 0.9123\n",
      "Epoch 7/10: Train Loss: 0.1809 | Val Loss: 0.2369 | Val Acc: 0.9140\n",
      "Epoch 8/10: Train Loss: 0.1618 | Val Loss: 0.2353 | Val Acc: 0.9151\n",
      "Epoch 9/10: Train Loss: 0.1440 | Val Loss: 0.2455 | Val Acc: 0.9151\n",
      "Epoch 10/10: Train Loss: 0.1283 | Val Loss: 0.2406 | Val Acc: 0.9181\n",
      "Training finished in 332.18 seconds.\n",
      "\n",
      "Models trained and saved. Ready for Step 6: RSA.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Setup and Data Loading\n",
    "train_loader, val_loader, test_loader = load_fashion_mnist() # This function was provided earlier\n",
    "\n",
    "# 2. Train the Reference Model\n",
    "print(\"--- Training Reference Network ---\")\n",
    "ref_model = ReferenceVisualNetwork()\n",
    "ref_model_trained, ref_history = train_model(ref_model, train_loader, val_loader)\n",
    "torch.save(ref_model_trained.state_dict(), 'reference_model.pth') # Save the model\n",
    "\n",
    "# 3. Train the Constrained Model\n",
    "print(\"\\n--- Training Constrained Network ---\")\n",
    "constrained_model = ConstrainedVisualNetwork() \n",
    "constrained_model_trained, const_history = train_model(constrained_model, train_loader, val_loader)\n",
    "torch.save(constrained_model_trained.state_dict(), 'constrained_model.pth') # Save the model\n",
    "\n",
    "print(\"\\nModels trained and saved. Ready for Step 6: RSA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7667b3",
   "metadata": {},
   "source": [
    "### Exercise 7 — Representational Similarity Analysis (RSA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e157a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def _get_subset(loader, n=1000):\n",
    "    xs, ys, c = [], [], 0\n",
    "    for x, y in loader:\n",
    "        xs.append(x); ys.append(y); c += x.size(0)\n",
    "        if c >= n: break\n",
    "    X = torch.cat(xs, dim=0)[:n]\n",
    "    Y = torch.cat(ys, dim=0)[:n]\n",
    "    return X, Y\n",
    "\n",
    "def collect_activations(model, data_tensor, device):\n",
    "    model.eval().to(device)\n",
    "    feats = {\"v1_v2\": [], \"ventral_fc\": [], \"dorsal_fc\": []}\n",
    "    bs = 128\n",
    "    for i in range(0, data_tensor.size(0), bs):\n",
    "        xb = data_tensor[i:i+bs].to(device)\n",
    "        _, f = model(xb)\n",
    "        for k in feats:\n",
    "            v = f[k].detach().cpu().numpy()\n",
    "            feats[k].append(v)\n",
    "    for k in feats:\n",
    "        feats[k] = np.concatenate(feats[k], axis=0)\n",
    "    return feats\n",
    "\n",
    "def rdm_vector(X):\n",
    "    Xc = X - X.mean(0, keepdims=True)\n",
    "    cov = Xc @ Xc.T\n",
    "    v = np.sqrt(np.clip(np.diag(cov), 1e-12, None))\n",
    "    corr = cov / (v[:, None] * v[None, :])\n",
    "    D = 1.0 - corr\n",
    "    iu = np.triu_indices_from(D, k=1)\n",
    "    return D[iu]\n",
    "\n",
    "def rsa_between(A, B):\n",
    "    a = rdm_vector(A)\n",
    "    b = rdm_vector(B)\n",
    "    s, _ = spearmanr(a, b)\n",
    "    return float(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63cacd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_2d(X):\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 1:\n",
    "        X = X[None, :]\n",
    "    elif X.ndim > 2:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "    return X\n",
    "\n",
    "def rdm_vector(X):\n",
    "    X = _to_2d(X).astype(np.float64, copy=False)\n",
    "    Xc = X - X.mean(0, keepdims=True)\n",
    "    cov = Xc @ Xc.T\n",
    "    v = np.sqrt(np.clip(np.diag(cov), 1e-12, None))\n",
    "    corr = cov / (v[:, None] * v[None, :])\n",
    "    D = 1.0 - corr\n",
    "    iu = np.triu_indices_from(D, 1)\n",
    "    return D[iu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f8faf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSA (Spearman) — Reference vs Constrained\n",
      "v1_v2 0.9869\n",
      "ventral_fc 0.8335\n",
      "dorsal_fc 0.5202\n"
     ]
    }
   ],
   "source": [
    "device = DEVICE\n",
    "X_test, y_test = _get_subset(test_loader, n=1000)\n",
    "\n",
    "ref_model_trained.eval().to(device)\n",
    "constrained_model_trained.eval().to(device)\n",
    "\n",
    "ref_feats = collect_activations(ref_model_trained, X_test, device)\n",
    "con_feats = collect_activations(constrained_model_trained, X_test, device)\n",
    "\n",
    "layers = [\"v1_v2\", \"ventral_fc\", \"dorsal_fc\"]\n",
    "rsa_scores = {l: rsa_between(ref_feats[l], con_feats[l]) for l in layers}\n",
    "\n",
    "print(\"RSA (Spearman) — Reference vs Constrained\")\n",
    "for l in layers:\n",
    "    print(l, round(rsa_scores[l], 4))\n",
    "\n",
    "np.savez(\"rsa_scores_ref_vs_constrained.npz\", **rsa_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef2129",
   "metadata": {},
   "source": [
    "### Exercise 8 — Functional connectivity before vs after training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83178559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΔFC mean |After−Before|\n",
      "Reference: 0.7812622246663143\n",
      "Constrained: 0.7674857501470256\n"
     ]
    }
   ],
   "source": [
    "def layerwise_fc(act_dict):\n",
    "    keys = [\"v1_v2\", \"ventral_fc\", \"dorsal_fc\"]\n",
    "    S = []\n",
    "    for k in keys:\n",
    "        X = act_dict[k].reshape(act_dict[k].shape[0], -1).mean(axis=1)\n",
    "        S.append(X)\n",
    "    S = np.stack(S, axis=0)\n",
    "    S = S - S.mean(1, keepdims=True)\n",
    "    C = np.corrcoef(S)\n",
    "    return keys, C\n",
    "\n",
    "def save_matrix(mat, title, path):\n",
    "    plt.figure()\n",
    "    plt.imshow(mat, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "X_small, y_small = _get_subset(test_loader, n=600)\n",
    "\n",
    "ref_untrained = ReferenceVisualNetwork().to(DEVICE).eval()\n",
    "con_untrained = ConstrainedVisualNetwork().to(DEVICE).eval()\n",
    "\n",
    "ref_feats_before = collect_activations(ref_untrained, X_small, DEVICE)\n",
    "ref_feats_after  = collect_activations(ref_model_trained, X_small, DEVICE)\n",
    "con_feats_before = collect_activations(con_untrained, X_small, DEVICE)\n",
    "con_feats_after  = collect_activations(constrained_model_trained, X_small, DEVICE)\n",
    "\n",
    "_, ref_C_before = layerwise_fc(ref_feats_before)\n",
    "_, ref_C_after  = layerwise_fc(ref_feats_after)\n",
    "_, con_C_before = layerwise_fc(con_feats_before)\n",
    "_, con_C_after  = layerwise_fc(con_feats_after)\n",
    "\n",
    "save_matrix(ref_C_before, \"Reference FC Before\", \"ref_fc_before.png\")\n",
    "save_matrix(ref_C_after,  \"Reference FC After\",  \"ref_fc_after.png\")\n",
    "save_matrix(con_C_before, \"Constrained FC Before\", \"con_fc_before.png\")\n",
    "save_matrix(con_C_after,  \"Constrained FC After\",  \"con_fc_after.png\")\n",
    "\n",
    "def tri_upper(m):\n",
    "    iu = np.triu_indices_from(m, 1)\n",
    "    return m[iu]\n",
    "\n",
    "print(\"ΔFC mean |After−Before|\")\n",
    "print(\"Reference:\", float(np.mean(np.abs(tri_upper(ref_C_after) - tri_upper(ref_C_before)))))\n",
    "print(\"Constrained:\", float(np.mean(np.abs(tri_upper(con_C_after) - tri_upper(con_C_before)))))\n",
    "\n",
    "np.savez(\"fc_matrices.npz\",\n",
    "         ref_before=ref_C_before, ref_after=ref_C_after,\n",
    "         con_before=con_C_before, con_after=con_C_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a711be1",
   "metadata": {},
   "source": [
    "### Exercise 9  — Empirical connectivity comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa1a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman(Empirical, Reference After): -0.5\n",
      "Spearman(Empirical, Constrained After): -0.5\n"
     ]
    }
   ],
   "source": [
    "emp = load_empirical_strengths()\n",
    "\n",
    "if len(emp) == 0:\n",
    "    print(\"No empirical CSVs found.\")\n",
    "else:\n",
    "    def pick_first(d, names):\n",
    "        for n in names:\n",
    "            df = d.get(n)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                return df\n",
    "        return None\n",
    "\n",
    "    def mean_conn(df, targets):\n",
    "        if df is None or df.empty:\n",
    "            return 0.0\n",
    "        cands = [c for c in df.columns if \"weight\" in c.lower() or \"strength\" in c.lower()]\n",
    "        if cands:\n",
    "            wcol = cands[0]\n",
    "        else:\n",
    "            nums = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if not nums:\n",
    "                return 0.0\n",
    "            wcol = nums[0]\n",
    "        sdf = df.astype(str)\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for t in targets:\n",
    "            mask = mask | sdf.apply(lambda r: r.str.contains(t, case=False, regex=False)).any(axis=1)\n",
    "        sel = df.loc[mask]\n",
    "        if sel.empty:\n",
    "            return 0.0\n",
    "        vals = pd.to_numeric(sel[wcol], errors=\"coerce\").dropna().abs()\n",
    "        if vals.empty:\n",
    "            return 0.0\n",
    "        return float(vals.mean())\n",
    "\n",
    "    V1_df      = pick_first(emp, [\"V1_left\", \"V1_right\"])\n",
    "    Ventral_df = pick_first(emp, [\"hOc4d_right\", \"hOc5_right\"])\n",
    "    Dorsal_df  = pick_first(emp, [\"hOc3d_left\"])\n",
    "\n",
    "    M = np.eye(3)\n",
    "    M[0,1] = M[1,0] = mean_conn(V1_df, [\"hOc4\", \"LOC\", \"hOc5\"])\n",
    "    M[0,2] = M[2,0] = mean_conn(V1_df, [\"hOc3\"])\n",
    "    M[1,2] = M[2,1] = mean_conn(Ventral_df, [\"hOc3\"])\n",
    "\n",
    "    save_matrix(M, \"Empirical 3-node FC (proxy)\", \"empirical_fc_proxy.png\")\n",
    "\n",
    "    f = np.load(\"fc_matrices.npz\")\n",
    "    ref_fc_after = f[\"ref_after\"]\n",
    "    con_fc_after = f[\"con_after\"]\n",
    "\n",
    "    def tri(m):\n",
    "        iu = np.triu_indices_from(m, 1)\n",
    "        return m[iu]\n",
    "\n",
    "    er = spearmanr(tri(M), tri(ref_fc_after))[0]\n",
    "    ec = spearmanr(tri(M), tri(con_fc_after))[0]\n",
    "    print(\"Spearman(Empirical, Reference After):\", round(float(er), 4))\n",
    "    print(\"Spearman(Empirical, Constrained After):\", round(float(ec), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a10f2",
   "metadata": {},
   "source": [
    "### Exercise 10 — t-SNE of representational space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "809069e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(feats, labels, title, path):\n",
    "    X = feats.reshape(feats.shape[0], -1)\n",
    "    idx = np.random.permutation(len(labels))[:1000]\n",
    "    Xs = X[idx]\n",
    "    ys = labels.numpy()[idx]\n",
    "    Z = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30).fit_transform(Xs)\n",
    "    plt.figure()\n",
    "    for c in np.unique(ys):\n",
    "        m = ys == c\n",
    "        plt.scatter(Z[m,0], Z[m,1], s=8, label=str(c))\n",
    "    plt.title(title)\n",
    "    plt.legend(markerscale=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "ref_pen = np.concatenate([ref_feats[\"ventral_fc\"], ref_feats[\"dorsal_fc\"]], axis=1)\n",
    "con_pen = np.concatenate([con_feats[\"ventral_fc\"], con_feats[\"dorsal_fc\"]], axis=1)\n",
    "\n",
    "tsne_plot(ref_pen, y_test, \"t-SNE Reference\", \"tsne_reference.png\")\n",
    "tsne_plot(con_pen, y_test, \"t-SNE Constrained\", \"tsne_constrained.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
